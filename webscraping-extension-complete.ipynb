{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Extension\n",
    "\n",
    "As an extension we'll be scraping Wikipedia to determine whether there is a link between hours of sunshine in a country and the rate of depression.\n",
    "\n",
    "Data about depression can be found here: https://en.wikipedia.org/wiki/Epidemiology_of_depression\n",
    "\n",
    "Data bout sun hours: https://en.wikipedia.org/wiki/List_of_cities_by_sunshine_duration\n",
    "\n",
    "We will proceed as follows:\n",
    "- Scrape 1st link to get DALY rate per country\n",
    "- Scrape 2nd link to get sun hours per city\n",
    "- Aggregate data to get sun hours per country\n",
    "- Filter data to match every country to its DALY rate and sun rate\n",
    "- Visualise data\n",
    "- Preprocess & model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few imports and constants we'll be needing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests import get\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "URL_DALY = 'https://en.wikipedia.org/wiki/Epidemiology_of_depression'\n",
    "URL_SUN = 'https://en.wikipedia.org/wiki/List_of_cities_by_sunshine_duration'\n",
    "FILE = './data-extension.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping DALY rate per country\n",
    "\n",
    "Our goal here is to get a dictionary containing every country mapped to its DALY rate. We start by getting the URL and creating a soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = get(URL_DALY)\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the rows of the table. We can safely ignore the first row as it contains the header of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.select('table > tbody > tr')[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the rows now extract the data into a dictionary which maps every country to its DALY rate. The `string.strip()` and `string.replace(..)` functions are useful to format the data as we want it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_daly = {}\n",
    "for row in rows:\n",
    "    vals = row.select('td')[1:]\n",
    "    \n",
    "    # Make sure values exist\n",
    "    if vals == []:\n",
    "        continue\n",
    "    \n",
    "    country = vals[0].text.strip()\n",
    "    daly = float(vals[1].text.replace(',', ''))\n",
    "    \n",
    "    data_daly[country] = daly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping sun hours per city\n",
    "We now need to scrape for sun hours. This is a bit more tricky since the data is spread across multiple tables and multiple cities. We start off by creating a mapping from country to a list of sun hours in all of its cities. For instance, say Botswana has 3 cities listed, we will have the following mapping: `'Botswana': [3330.0, 3371.0, 3579.0]`. We'll then aggregate all the data to get a single averaged value per country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by collecting all the tables of the page into a single array with all the rows (i.e. all the `td` tags). Note that the first two tables of the page do not contain any data we want and can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = get(URL_SUN)\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "tables = soup.select('table > tbody')[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now aggregate all the rows from the tables into a single array of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_sun = []\n",
    "for table in tables:\n",
    "    rows = table.select('tr')[1:]\n",
    "    rows_sun += rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a mapping from country to an array of sun hours per city (as mentioned above). Note that the sunhours per year can be found in column 14 (starting at 0). Again, it can be useful to use the `string.replace(..)` function to format number data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_cities = {}\n",
    "for row in rows_sun:\n",
    "    vals = row.select('td')\n",
    "    country = vals[0].text.replace('\\n', '')\n",
    "    sun = float(vals[14].text.replace(',', ''))\n",
    "    \n",
    "    # If we already have data for the country, we append to the array\n",
    "    if country in sun_cities:\n",
    "        sun_cities[country].append(sun)\n",
    "    else: # Oterwise, we create a new array with the data\n",
    "        sun_cities[country] = [sun]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating data to hours per country\n",
    "\n",
    "We can now aggregate the data from sun hours per city to sun hours per country by just taking the mean of every city for every country. This will give us our final mapping from country to sun hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sun = {}\n",
    "for country, cities in sun_cities.items():\n",
    "    data_sun[country] = sum(cities) / len(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data to match every country to its DALY rate and sun rate\n",
    "Now that we have two mappings `'country': daly` and `'country': 'sun'`, we wich to aggregate the data into the following format `[[country, daly, sun], [country, daly, sun], ..]`. This will make it easier to work with the data after. Note that there may be cases of missing data for some countries. In that case we should just skip those and keep countries which both have a DALY rate and a sunhours entry. We can therefore start by getting the list of countries that have data for both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of countries that both have a DALY rate and sunhours entry\n",
    "countries_daly = set(data_daly.keys())\n",
    "countries_sun = set(data_sun.keys())\n",
    "countries = countries_daly & countries_sun # Use set intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the dataset following the format described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for country in countries:\n",
    "    data.append([country, data_daly[country], data_sun[country]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put our data into a pandas dataframe and save it for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns=['country', 'daly', 'sunhours'])\n",
    "# Save data to file at path FILE\n",
    "data.to_csv(FILE, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simply load our data from a file instead of web scraping again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise data\n",
    "Now that we have our fancy data we can use matplotlib to plot a few graphs. We can start off by plotting daly rate for sunhours. We use `plt.scatter(..)` to display points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel('sunhours')\n",
    "plt.ylabel('daly')\n",
    "plt.scatter(data['sunhours'], data['daly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess & model data\n",
    "Now that we have our data, we can normalise it and train a model on it. We start off by importing some libraries and normalising the data to have it fit in a `0-1` range. For reference, the min-max normalisation formula is $X_{normalised}=\\frac{X-min(X)}{max(X)-min(X)}$. Note that it might be useful to drop the country name from the data and just keep the daly and sun rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = data[['daly', 'sunhours']]\n",
    "data = (data - data.min()) / (data.max() - data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into `X` and `y`, respectively for input features and output feature. We have a single input which is the sunhours and our output feature is the daly rate we want to predict. We also need to convert our data from a pandas dataframe to a numpy array using the `to_numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['sunhours'].to_numpy().reshape(-1, 1)\n",
    "y = data['daly'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn's `train_test_split` function we now spearate our data into a training and test set to end up with `X_train`, `y_train`, `X_test`, `y_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size=0.9,\n",
    "    test_size=0.1,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally let's fit a simple linear regression model to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X=X_train, y=y_train)\n",
    "reg.score(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_William Profit (williamprofit.com) on behalf of ICDSS (icdss.uk)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
